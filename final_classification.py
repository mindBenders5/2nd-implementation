# -*- coding: utf-8 -*-
"""Final_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AEYpYHoLi-hY_bPd4oC0yaObpMuAD8yT
"""

from google.colab import drive
drive.mount('/content/drive')

import json
import os
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torch
import torchvision.transforms as T
from torchvision.models.detection import ssdlite320_mobilenet_v3_large
import matplotlib.pyplot as plt

# Define paths
images_root_dir = '/content/drive/MyDrive/Current_Dataset/Splitted_Images'
annotations_file = '/content/drive/MyDrive/Current_Dataset/annotations/instances_Train.json'

# Load full annotations JSON
with open(annotations_file, 'r') as f:
    annotations = json.load(f)

# Define paths for each split
splits = {
    'train': os.path.join(images_root_dir, 'train'),
    'val': os.path.join(images_root_dir, 'val'),
    'test': os.path.join(images_root_dir, 'test')
}

# Filter annotations for each split based on images in the respective folders
def filter_annotations_for_split(split_folder, annotations):
    image_filenames = set(os.listdir(split_folder))
    split_annotations = {
        'images': [img for img in annotations['images'] if os.path.basename(img['file_name']) in image_filenames],
        'annotations': []
    }
    image_ids = {img['id'] for img in split_annotations['images']}
    split_annotations['annotations'] = [ann for ann in annotations['annotations'] if ann['image_id'] in image_ids]
    return split_annotations

class CoralDataset(Dataset):
    def __init__(self, images_folder, annotations, transforms=None):
        self.images_folder = images_folder
        self.annotations = annotations
        self.transforms = transforms or T.Compose([T.ToTensor()])

    def __len__(self):
        return len(self.annotations['images'])

    def __getitem__(self, idx):
        img_info = self.annotations['images'][idx]
        img_path = os.path.join(self.images_folder, os.path.basename(img_info['file_name']))
        image = Image.open(img_path).convert("RGB")
        image = self.transforms(image)

        img_id = img_info['id']
        boxes = []
        labels = []
        for ann in self.annotations['annotations']:
            if ann['image_id'] == img_id:
                bbox = ann['bbox']
                x_min, y_min, width, height = bbox
                x_max = x_min + width
                y_max = y_min + height
                boxes.append([x_min, y_min, x_max, y_max])
                labels.append(ann['category_id'])

        target = {
            'boxes': torch.tensor(boxes, dtype=torch.float32),
            'labels': torch.tensor(labels, dtype=torch.int64)
        }

        return image, target

datasets = {}
data_loaders = {}

for split, folder in splits.items():
    split_annotations = filter_annotations_for_split(folder, annotations)
    datasets[split] = CoralDataset(folder, split_annotations)
    data_loaders[split] = DataLoader(datasets[split], batch_size=4, shuffle=(split == 'train'), collate_fn=lambda x: tuple(zip(*x)))

print("Data loaders for train, val, and test splits are ready.")

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize SSD with MobileNetV3 backbone
num_classes = 6  # 5 coral species + background
model = ssdlite320_mobilenet_v3_large(weights="DEFAULT")
model.head.classification_head.num_classes = num_classes
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# Training Loop for SSD with MobileNetV3
num_epochs = 50
unfreeze_epoch = 5  # Unfreeze backbone after a few epochs

for epoch in range(num_epochs):
    # Unfreeze backbone after a few epochs
    if epoch == unfreeze_epoch:
        for param in model.backbone.parameters():
            param.requires_grad = True

    # Training
    model.train()
    total_loss = 0.0
    for images, targets in data_loaders['train']:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        optimizer.zero_grad()
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        losses.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += losses.item()

    # Validation (skip loss calculation)
    model.eval()
    with torch.no_grad():
        for images, _ in data_loaders['val']:  # Only images, skip targets for validation
            images = [img.to(device) for img in images]
            predictions = model(images)  # Get predictions instead of losses

    # Output metrics
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(data_loaders['train']):.4f}")

# Define the path to save the model
model_save_path = '/content/drive/MyDrive/Current_Dataset/ssd_mobilenet_v3_coralN.pth'

# Save model and optimizer state dictionaries
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,  # Save the last epoch number
}, model_save_path)

print(f"Model saved to {model_save_path}")

import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large

# Define device (GPU or CPU)
device = torch.load with map_location=torch.device('cpu')

# Initialize the model (same architecture as during training)
num_classes = 6  # 5 coral species + background
model = ssdlite320_mobilenet_v3_large(weights="DEFAULT")
model.head.classification_head.num_classes = num_classes

# Load the saved model state
model_save_path = '/content/drive/MyDrive/Current_Dataset/ssd_mobilenet_v3_coralN.pth'
checkpoint = torch.load(model_save_path)

# Load model weights and optimizer state into the model
model.load_state_dict(checkpoint['model_state_dict'])

# Move model to GPU (if available)
model.to(device)

# Set the model to evaluation mode
model.eval()

print(f"Model loaded successfully on {device}.")

from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define the transformations for the test set (use same transforms as training if needed)
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Adjust to your model's input size
    transforms.ToTensor(),
])

# Load the test dataset
test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Current_Dataset/Splitted_Images', transform=transform)  # Update path

# Define the test DataLoader
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Set batch_size as needed

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision.transforms import functional as F

def visualize_predictions(image, predictions, label_map):
    # Convert the image tensor to a PIL image and then to RGB format
    image_np = F.to_pil_image(image).convert("RGB")

    # Adjust the figure size for smaller images
    plt.figure(figsize=(5, 5))  # Reduced size for clearer, smaller display
    plt.imshow(image_np)

    # Create a drawing context
    ax = plt.gca()

    for pred in predictions:
        boxes = pred['boxes'].cpu().numpy()
        labels = pred['labels'].cpu().numpy()
        scores = pred['scores'].cpu().numpy()

        # Draw each box and label if score exceeds a threshold (e.g., 0.5)
        for box, label, score in zip(boxes, labels, scores):
            if score >= 0.5:  # Only show confident predictions
                x_min, y_min, x_max, y_max = box

                # Create a rectangle patch for the bounding box
                rect = patches.Rectangle(
                    (x_min, y_min),
                    x_max - x_min,
                    y_max - y_min,
                    linewidth=2,
                    edgecolor='red',
                    facecolor='none'
                )
                ax.add_patch(rect)

                # Add the label text
                label_name = label_map.get(label, f"Class {label}")
                ax.text(
                    x_min,
                    y_min - 10,
                    f"{label_name}: {score:.2f}",
                    color="red",
                    fontsize=8,  # Reduced font size for clarity in smaller images
                    backgroundcolor="white"
                )

    plt.axis('off')
    plt.show()

# Define the label map
label_map = {
    1: 'Cabbage Coral',
    2: 'Porites',
    3: 'Branching Coral',
    4: 'Plate Coral',
    5: 'Brain Coral'
}
# Set the model to evaluation mode
model.eval()

# Iterate through the test dataset and visualize predictions
with torch.no_grad():
    for images, _ in test_loader:
        images = [img.to(device) for img in images]
        predictions = model(images)

        # Loop through each image and corresponding prediction
        for image, prediction in zip(images, predictions):
            visualize_predictions(image, [prediction], label_map)

import torch

# Define the path to the saved model
model_save_path = '/content/drive/MyDrive/Current_Dataset/ssd_mobilenet_v3_coralN.pth'

# Initialize the model
model = ssdlite320_mobilenet_v3_large(weights="DEFAULT")
num_classes = 6  # 5 coral species + background
model.head.classification_head.num_classes = num_classes

# Load the saved model state dict
checkpoint = torch.load(model_save_path, map_location=torch.device('cpu'))  # Ensure it's loaded onto the CPU

# Load model state dict
model.load_state_dict(checkpoint['model_state_dict'])

# Optionally load the optimizer state dict if you plan to continue training
# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Set the model to evaluation mode
model.eval()

print("Model loaded successfully on CPU.")

from PIL import Image  # Ensure this is imported
import os
import json
import torch
import numpy as np
from collections import defaultdict
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

# Load your annotations file
annotations_file = '/content/drive/MyDrive/Current_Dataset/annotations/instances_Train.json'
with open(annotations_file, 'r') as f:
    annotations = json.load(f)

# Custom dataset class for test data
class CoralImageDataset(Dataset):
    def __init__(self, root_dir, annotations, all_images_dir, transform=None):
        self.root_dir = root_dir  # Test folder
        self.annotations = annotations
        self.transform = transform
        self.all_images_dir = all_images_dir  # Folder containing all images

        # Get available test image filenames
        available_images = set(os.listdir(root_dir))

        # Filter annotations to only include those in the test folder
        self.image_filenames = [
            img['file_name'].split('/')[-1]  # Get the base filename
            for img in annotations['images']
            if img['file_name'].split('/')[-1] in available_images
        ]
        self.image_ids = {img['file_name'].split('/')[-1]: img['id'] for img in annotations['images']}
        self.ground_truth_boxes = defaultdict(list)

        # Build a dictionary of ground truth boxes by image ID
        for ann in annotations['annotations']:
            self.ground_truth_boxes[ann['image_id']].append(ann['bbox'])

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = self.image_filenames[idx]
        img_path = os.path.join(self.all_images_dir, img_name)  # Fetch from all images directory
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Get ground truth boxes for this image
        image_id = self.image_ids[img_name]
        true_boxes = self.ground_truth_boxes.get(image_id, [])

        return image, true_boxes, img_name

# Define the IoU calculation function
def calculate_iou(pred_box, true_box):
    x_min_inter = max(pred_box[0], true_box[0])
    y_min_inter = max(pred_box[1], true_box[1])
    x_max_inter = min(pred_box[2], true_box[2])
    y_max_inter = min(pred_box[3], true_box[3])

    if x_max_inter < x_min_inter or y_max_inter < y_min_inter:
        return 0.0

    inter_area = (x_max_inter - x_min_inter) * (y_max_inter - y_min_inter)
    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])
    true_area = (true_box[2] - true_box[0]) * (true_box[3] - true_box[1])
    union_area = pred_area + true_area - inter_area
    iou = inter_area / union_area
    return iou

# Evaluate the model
def evaluate_model(model, test_loader, annotations, device, threshold=0.5):
    model.eval()
    all_ious = []
    num_predictions = 0
    num_true_boxes = 0

    # Create a dictionary for ground truth annotations
    test_image_ids = {img['id'] for img in annotations['images'] if os.path.basename(img['file_name']) in os.listdir(test_loader.dataset.root_dir)}
    ground_truth_boxes = defaultdict(list)

    for ann in annotations['annotations']:
        if ann['image_id'] in test_image_ids:
            ground_truth_boxes[ann['image_id']].append(ann['bbox'])

    with torch.no_grad():
        for images, true_boxes, img_names in test_loader:
            images = [img.to(device) for img in images]
            predictions = model(images)

            for i, prediction in enumerate(predictions):
                pred_boxes = prediction['boxes'].cpu().numpy()
                pred_scores = prediction['scores'].cpu().numpy()

                high_confidence_preds = [
                    (box, score) for box, score in zip(pred_boxes, pred_scores) if score >= threshold
                ]

                image_id = test_loader.dataset.image_ids[img_names[i]]
                true_boxes_for_image = ground_truth_boxes.get(image_id, [])

                for pred_box, _ in high_confidence_preds:
                    best_iou = 0.0
                    for true_box in true_boxes_for_image:
                        iou = calculate_iou(pred_box, true_box)
                        best_iou = max(best_iou, iou)

                    all_ious.append(best_iou)
                    num_predictions += 1
                    num_true_boxes += len(true_boxes_for_image)

    avg_iou = np.mean(all_ious) if all_ious else 0
    return avg_iou, num_predictions, num_true_boxes

# Define the transformations for the test set
transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])

# Define paths
test_dir = '/content/drive/MyDrive/Current_Dataset/Splitted_Images/test'
all_images_dir = '/content/drive/MyDrive/Current_Dataset/Images/Images'

# Load the test dataset
test_dataset = CoralImageDataset(root_dir=test_dir, annotations=annotations, all_images_dir=all_images_dir, transform=transform)

# Define the test DataLoader
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

# Run the evaluation
avg_iou, num_predictions, num_true_boxes = evaluate_model(model, test_loader, annotations, device)
print(f"Average IoU: {avg_iou:.4f}")
print(f"Number of Predictions: {num_predictions}")
print(f"Number of True Boxes: {num_true_boxes}")

from sklearn.metrics import confusion_matrix

# Updated function to evaluate and compute the confusion matrix
def evaluate_model_with_cm(model, test_loader, annotations, device, threshold=0.5):
    model.eval()
    all_preds = []  # To store predicted labels
    all_labels = []  # To store true labels
    all_ious = []  # List to store IoU scores
    num_predictions = 0
    num_true_boxes = 0

    # Create a dictionary for ground truth annotations
    test_image_ids = {img['id'] for img in annotations['images'] if os.path.basename(img['file_name']) in os.listdir(test_loader.dataset.root_dir)}
    ground_truth_boxes = defaultdict(list)
    ground_truth_labels = defaultdict(list)  # Store ground truth labels

    for ann in annotations['annotations']:
        if ann['image_id'] in test_image_ids:
            ground_truth_boxes[ann['image_id']].append(ann['bbox'])
            ground_truth_labels[ann['image_id']].append(ann['category_id'])  # Assuming category_id is the class label

    with torch.no_grad():
        for images, true_boxes, img_names in test_loader:
            images = [img.to(device) for img in images]
            predictions = model(images)

            for i, prediction in enumerate(predictions):
                pred_boxes = prediction['boxes'].cpu().numpy()
                pred_labels = prediction['labels'].cpu().numpy()  # Predicted labels
                pred_scores = prediction['scores'].cpu().numpy()

                # Filter predictions by score threshold
                high_confidence_preds = [
                    (box, label, score) for box, label, score in zip(pred_boxes, pred_labels, pred_scores) if score >= threshold
                ]

                img_name = img_names[i]
                image_id = test_loader.dataset.image_ids[img_name]
                true_boxes_for_image = ground_truth_boxes.get(image_id, [])
                true_labels_for_image = ground_truth_labels.get(image_id, [])

                for pred_box, pred_label, _ in high_confidence_preds:
                    best_iou = 0.0
                    best_gt_idx = -1

                    # Find the best matching ground truth box
                    for gt_idx, true_box in enumerate(true_boxes_for_image):
                        iou = calculate_iou(pred_box, true_box)
                        if iou > best_iou:
                            best_iou = iou
                            best_gt_idx = gt_idx

                    # If the best IoU is above the threshold, count it as a true positive
                    if best_iou >= threshold:
                        all_preds.append(pred_label)
                        all_labels.append(true_labels_for_image[best_gt_idx])
                        all_ious.append(best_iou)
                        num_predictions += 1
                        num_true_boxes += len(true_boxes_for_image)
                    else:
                        # If no match, consider it as a false positive
                        all_preds.append(pred_label)
                        all_labels.append(-1)  # Assuming -1 represents a false positive

                # Handle false negatives (ground truth boxes not matched with any prediction)
                for gt_idx, true_box in enumerate(true_boxes_for_image):
                    if best_iou < threshold:
                        all_preds.append(-1)  # Assuming -1 represents a false positive or unmatched label
                        all_labels.append(true_labels_for_image[gt_idx])

    # Calculate the confusion matrix
    cm = confusion_matrix(all_labels, all_preds, labels=sorted(set(all_labels + all_preds)))

    avg_iou = np.mean(all_ious) if all_ious else 0
    precision = num_predictions / (num_predictions + sum(all_preds.count(label) for label in set(all_preds)))
    recall = num_predictions / (num_true_boxes + sum(all_labels.count(label) for label in set(all_labels)))
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return avg_iou, precision, recall, f1_score, cm

# Run the evaluation and compute the confusion matrix
avg_iou, precision, recall, f1_score, cm = evaluate_model_with_cm(model, test_loader, annotations, device)

# Print evaluation results
print(f"Average IoU: {avg_iou:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")
print("Confusion Matrix:")
print(cm)

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from sklearn.metrics import confusion_matrix
from torchvision import transforms
from PIL import Image

# Updated function to evaluate and compute the confusion matrix
def evaluate_model_with_cm(model, test_loader, annotations, device, threshold=0.5):
    model.eval()
    all_preds = []  # To store predicted labels
    all_labels = []  # To store true labels
    all_ious = []  # List to store IoU scores
    num_predictions = 0
    num_true_boxes = 0

    # Create a dictionary for ground truth annotations
    test_image_ids = {img['id'] for img in annotations['images'] if os.path.basename(img['file_name']) in os.listdir(test_loader.dataset.root_dir)}
    ground_truth_boxes = defaultdict(list)
    ground_truth_labels = defaultdict(list)  # Store ground truth labels

    for ann in annotations['annotations']:
        if ann['image_id'] in test_image_ids:
            ground_truth_boxes[ann['image_id']].append(ann['bbox'])
            ground_truth_labels[ann['image_id']].append(ann['category_id'])  # Assuming category_id is the class label

    with torch.no_grad():
        for images, true_boxes, img_names in test_loader:
            images = [img.to(device) for img in images]
            predictions = model(images)

            for i, prediction in enumerate(predictions):
                pred_boxes = prediction['boxes'].cpu().numpy()
                pred_labels = prediction['labels'].cpu().numpy()  # Predicted labels
                pred_scores = prediction['scores'].cpu().numpy()

                # Filter predictions by score threshold
                high_confidence_preds = [
                    (box, label, score) for box, label, score in zip(pred_boxes, pred_labels, pred_scores) if score >= threshold
                ]

                img_name = img_names[i]
                image_id = test_loader.dataset.image_ids[img_name]
                true_boxes_for_image = ground_truth_boxes.get(image_id, [])
                true_labels_for_image = ground_truth_labels.get(image_id, [])

                for pred_box, pred_label, _ in high_confidence_preds:
                    best_iou = 0.0
                    best_gt_idx = -1

                    # Find the best matching ground truth box
                    for gt_idx, true_box in enumerate(true_boxes_for_image):
                        iou = calculate_iou(pred_box, true_box)
                        if iou > best_iou:
                            best_iou = iou
                            best_gt_idx = gt_idx

                    # If the best IoU is above the threshold, count it as a true positive
                    if best_iou >= threshold:
                        all_preds.append(pred_label)
                        all_labels.append(true_labels_for_image[best_gt_idx])
                        all_ious.append(best_iou)
                        num_predictions += 1
                        num_true_boxes += len(true_boxes_for_image)
                    else:
                        # If no match, consider it as a false positive
                        all_preds.append(pred_label)
                        all_labels.append(-1)  # Assuming -1 represents a false positive

                # Handle false negatives (ground truth boxes not matched with any prediction)
                for gt_idx, true_box in enumerate(true_boxes_for_image):
                    if best_iou < threshold:
                        all_preds.append(-1)  # Assuming -1 represents a false positive or unmatched label
                        all_labels.append(true_labels_for_image[gt_idx])

    # Calculate the confusion matrix
    cm = confusion_matrix(all_labels, all_preds, labels=sorted(set(all_labels + all_preds)))

    avg_iou = np.mean(all_ious) if all_ious else 0
    precision = num_predictions / (num_predictions + sum(all_preds.count(label) for label in set(all_preds)))
    recall = num_predictions / (num_true_boxes + sum(all_labels.count(label) for label in set(all_labels)))
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return avg_iou, precision, recall, f1_score, cm

# Plot the confusion matrix as a heatmap
def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Run the evaluation and compute the confusion matrix
avg_iou, precision, recall, f1_score, cm = evaluate_model_with_cm(model, test_loader, annotations, device)

# Print evaluation results
print(f"Average IoU: {avg_iou:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")
print("Confusion Matrix:")
print(cm)

# Get the class names (adjust this with your actual class names)
class_names = ['Cabbage_Coral', 'Porites', 'Branching_Coral', 'Plate_Coral','Brain_Coral']  # Replace these with your actual class names

# Plot the confusion matrix as a heatmap
plot_confusion_matrix(cm, class_names)

import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large

# Define the path to the saved model
model_save_path = '/content/drive/MyDrive/Current_Dataset/ssd_mobilenet_v3_coralN.pth'

# Initialize the model
model = ssdlite320_mobilenet_v3_large(weights="DEFAULT")
num_classes = 6  # 5 coral species + background
model.head.classification_head.num_classes = num_classes

# Load the saved model state dict
checkpoint = torch.load(model_save_path, map_location=torch.device('cpu'))  # Ensure it's loaded onto the CPU

# Load the model state dict
model.load_state_dict(checkpoint['model_state_dict'])

# Optionally load the optimizer state dict if you plan to continue training
# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Set the model to evaluation mode
model.eval()

print("Model loaded successfully on CPU.")

import os
import cv2
import torch
import numpy as np
from torchvision import transforms
from PIL import Image
from collections import defaultdict

# Define the transformation for the input image (resize and normalization)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# Function to extract frames from video
def extract_frames_from_video(video_path, output_dir):
    video = cv2.VideoCapture(video_path)
    frame_count = 0
    frames = []

    if not video.isOpened():
        print(f"Error opening video file: {video_path}")
        return frames

    while True:
        ret, frame = video.read()
        if not ret:
            break
        frame_filename = os.path.join(output_dir, f'frame_{frame_count:04d}.png')
        cv2.imwrite(frame_filename, frame)
        frames.append(frame_filename)
        frame_count += 1

    video.release()
    print(f"Extracted {frame_count} frames.")
    return frames

# Function to detect coral species in frames and add bounding boxes
def detect_coral_in_frames(frames, model, device, threshold=0.5):
    predictions = []

    for frame_path in frames:
        image = Image.open(frame_path).convert('RGB')
        image_tensor = transform(image).unsqueeze(0).to(device)

        with torch.no_grad():
            prediction = model(image_tensor)

        # Get the boxes, labels, and scores from the prediction
        boxes = prediction[0]['boxes'].cpu().numpy()
        labels = prediction[0]['labels'].cpu().numpy()
        scores = prediction[0]['scores'].cpu().numpy()

        # Filter predictions by score threshold
        high_confidence_preds = [
            (box, label, score) for box, label, score in zip(boxes, labels, scores) if score >= threshold
        ]

        predictions.append((frame_path, high_confidence_preds))

    return predictions

# Function to annotate frames with bounding boxes and labels
def annotate_frame(frame_path, predictions):
    frame = cv2.imread(frame_path)

    if frame is None:
        print(f"Error reading frame: {frame_path}")
        return frame  # Return None if frame couldn't be read

    for box, label, score in predictions:
        x1, y1, x2, y2 = box
        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        cv2.putText(frame, f'Label: {label}, Score: {score:.2f}',
                    (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    return frame

# Function to combine frames back into a video
def combine_frames_to_video(frames, output_video_path, fps=30):
    # Ensure frames are valid before proceeding
    if not frames:
        print("No frames to combine.")
        return

    # Read the first frame to get size
    frame = cv2.imread(frames[0])
    if frame is None:
        print(f"Error reading the first frame: {frames[0]}")
        return

    height, width, _ = frame.shape

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    for frame_path in frames:
        frame = cv2.imread(frame_path)
        if frame is not None:
            video_writer.write(frame)

    video_writer.release()
    print(f"Video saved to: {output_video_path}")

# Main function to process the video
def process_video(input_video_path, output_video_path, output_frames_dir, model, device):
    # Step 1: Extract frames from the input video
    frames = extract_frames_from_video(input_video_path, output_frames_dir)

    # Step 2: Detect coral species in frames
    predictions = detect_coral_in_frames(frames, model, device, threshold=0.5)

    # Step 3: Annotate frames with bounding boxes and labels
    annotated_frames = []
    annotated_frames_dir = output_frames_dir + '_annotated'
    os.makedirs(annotated_frames_dir, exist_ok=True)

    for frame_path, preds in predictions:
        annotated_frame = annotate_frame(frame_path, preds)

        if annotated_frame is not None:
            annotated_frame_path = frame_path.replace(output_frames_dir, annotated_frames_dir)
            cv2.imwrite(annotated_frame_path, annotated_frame)
            annotated_frames.append(annotated_frame_path)

    # Step 4: Combine annotated frames back into a video
    combine_frames_to_video(annotated_frames, output_video_path, fps=30)
    print(f'Annotated video saved to: {output_video_path}')

# Paths to input video and output
input_video_path = '/content/drive/MyDrive/Video19.mp4'
output_video_path = '/content/drive/MyDrive/annotated_video19.mp4'
output_frames_dir = '/content/drive/MyDrive/frames'

# Ensure that the output directory for frames exists
os.makedirs(output_frames_dir, exist_ok=True)

# Run the video processing pipeline
process_video(input_video_path, output_video_path, output_frames_dir, model, device)

import os
frames_dir = '/content/drive/MyDrive/frames'
print(os.listdir(frames_dir))  # Ensure that frames are listed here